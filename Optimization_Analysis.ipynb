{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbg27a6YJfm3KF1Ghu7HBz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yigitaytt/Deep-Learning-Optimization-Methods-Python/blob/main/Optimization_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hYHhmnXawO_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2.0\""
      ],
      "metadata": {
        "id": "gGqNIxAqwL_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzHdN4t8vXr9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import time\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup and Data Loading\n",
        "Loads the processed dataset and prepares PyTorch tensors."
      ],
      "metadata": {
        "id": "HdvJhu2Wva9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device set to: {device}\")\n",
        "\n",
        "print(\"1. Loading Data...\")\n",
        "try:\n",
        "    # Keeping filenames as is to match your uploaded files\n",
        "    X_train = np.load(\"egitim_X.npy\")\n",
        "    y_train = np.load(\"egitim_y.npy\")\n",
        "    X_test = np.load(\"test_X.npy\")\n",
        "    y_test = np.load(\"test_y.npy\")\n",
        "except FileNotFoundError:\n",
        "    print(\"ERROR: Data files not found. Please upload 'egitim_X.npy', etc.\")\n",
        "    # Exit command removed for Colab compatibility, using raise instead\n",
        "    raise SystemExit(\"Stopping execution due to missing files.\")\n",
        "\n",
        "# Convert to Tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "y_train_tensor = torch.FloatTensor(y_train).view(-1, 1).to(device)\n",
        "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "y_test_tensor = torch.FloatTensor(y_test).view(-1, 1).to(device)\n",
        "\n",
        "print(f\"Training Set Shape: {X_train_tensor.shape}\")\n",
        "print(f\"Test Set Shape: {X_test_tensor.shape}\")"
      ],
      "metadata": {
        "id": "f9S9YdD5veHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Regression Model Architecture\n",
        "Defines a simple linear regression model with Tanh activation."
      ],
      "metadata": {
        "id": "UGf2FnCuvpmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        # Linear layer (bias is implicitly handled if data has bias column,\n",
        "        # but here we set bias=False based on your original code)\n",
        "        self.linear = nn.Linear(input_dim, 1, bias=False)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.linear(x))"
      ],
      "metadata": {
        "id": "Hby4N_lovs_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Optimization Experiments (GD, SGD, Adam)\n",
        "Runs training loops with different optimizers across multiple random seeds."
      ],
      "metadata": {
        "id": "L-eRxX78v1-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiments():\n",
        "    input_dim = X_train.shape[1]\n",
        "    seeds = [1, 42, 100, 555, 999]  # 5 fixed seeds for reproducibility\n",
        "    algorithms = ['GD', 'SGD', 'Adam']\n",
        "\n",
        "    # Dictionary to store results\n",
        "    results = {\n",
        "        alg: {\n",
        "            'train_loss': [], 'test_loss': [],\n",
        "            'train_acc': [], 'test_acc': [],\n",
        "            'weights': [], 'time_log': []\n",
        "        }\n",
        "        for alg in algorithms\n",
        "    }\n",
        "\n",
        "    for seed_idx, seed in enumerate(seeds):\n",
        "        print(f\"\\n>>> Processing Seed {seed} ({seed_idx+1}/{len(seeds)})...\")\n",
        "\n",
        "        # Set seed for reproducibility\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        # Initialize a temporary model to get consistent starting weights for all algos\n",
        "        temp_model = RegressionModel(input_dim)\n",
        "        initial_weights = temp_model.linear.weight.data.clone()\n",
        "\n",
        "        for alg in algorithms:\n",
        "            # Re-initialize model\n",
        "            model = RegressionModel(input_dim).to(device)\n",
        "            # Force same initial weights\n",
        "            model.linear.weight.data = initial_weights.clone().to(device)\n",
        "\n",
        "            # Hyperparameters based on algorithm\n",
        "            if alg == 'GD':\n",
        "                optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
        "                batch_size = len(X_train) # Full batch\n",
        "                epochs = 100\n",
        "            elif alg == 'SGD':\n",
        "                optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
        "                batch_size = 1 # Stochastic\n",
        "                epochs = 100\n",
        "            elif alg == 'Adam':\n",
        "                optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "                batch_size = len(X_train)\n",
        "                epochs = 100\n",
        "\n",
        "            criterion = nn.MSELoss()\n",
        "\n",
        "            # Metric tracking lists for current seed/algo\n",
        "            loss_tr, loss_te, acc_tr, acc_te, w_hist, t_log = [], [], [], [], [], []\n",
        "\n",
        "            # Record initial weights\n",
        "            w_hist.append(model.linear.weight.data.cpu().numpy().flatten().copy())\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                model.train()\n",
        "                perm = torch.randperm(len(X_train)) # Shuffle data\n",
        "                last_batch_loss = 0.0\n",
        "\n",
        "                # Mini-batch loop\n",
        "                for i in range(0, len(X_train), batch_size):\n",
        "                    indices = perm[i:i+batch_size]\n",
        "                    batch_x, batch_y = X_train_tensor[indices], y_train_tensor[indices]\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(batch_x)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    last_batch_loss = loss.item()\n",
        "\n",
        "                elapsed = time.time() - start_time\n",
        "\n",
        "                # Evaluation phase\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    l_tr = last_batch_loss\n",
        "\n",
        "                    # Calculate Accuracy (Sign Check)\n",
        "                    out_tr_acc = model(X_train_tensor)\n",
        "                    a_tr = (torch.sign(out_tr_acc) == y_train_tensor).float().mean().item()\n",
        "\n",
        "                    out_te = model(X_test_tensor)\n",
        "                    l_te = criterion(out_te, y_test_tensor).item()\n",
        "                    a_te = (torch.sign(out_te) == y_test_tensor).float().mean().item()\n",
        "\n",
        "                    # Store metrics\n",
        "                    loss_tr.append(l_tr)\n",
        "                    loss_te.append(l_te)\n",
        "                    acc_tr.append(a_tr)\n",
        "                    acc_te.append(a_te)\n",
        "                    t_log.append(elapsed)\n",
        "                    w_hist.append(model.linear.weight.data.cpu().numpy().flatten().copy())\n",
        "\n",
        "            # Append run results to main dictionary\n",
        "            results[alg]['train_loss'].append(loss_tr)\n",
        "            results[alg]['test_loss'].append(loss_te)\n",
        "            results[alg]['train_acc'].append(acc_tr)\n",
        "            results[alg]['test_acc'].append(acc_te)\n",
        "            results[alg]['weights'].append(w_hist)\n",
        "            results[alg]['time_log'].append(t_log)\n",
        "\n",
        "            print(f\"   -> {alg} completed. (Final Test Acc: {acc_te[-1]:.2f})\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the experiments\n",
        "print(\"Starting Experiments...\")\n",
        "results = run_experiments()\n",
        "\n",
        "# Save results for Part B (T-SNE)\n",
        "np.save(\"model_results_part_A.npy\", results)\n",
        "print(\"Results saved to 'model_results_part_A.npy'\")"
      ],
      "metadata": {
        "id": "2aohXXR4vvmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Visualization of Metrics\n",
        "Plots Training/Test Loss and Accuracy against Epochs and Time."
      ],
      "metadata": {
        "id": "rrUeoBzEv9Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(metric_key, title, ylabel, x_axis='epoch'):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    colors = {'GD': 'blue', 'SGD': 'orange', 'Adam': 'green'}\n",
        "\n",
        "    for alg in ['GD', 'SGD', 'Adam']:\n",
        "        data = results[alg][metric_key]\n",
        "\n",
        "        # Determine X-axis data\n",
        "        if x_axis == 'time':\n",
        "            x_data_list = results[alg]['time_log']\n",
        "        else:\n",
        "            x_data_list = [list(range(len(d))) for d in data]\n",
        "\n",
        "        # Truncate to minimum length to match dimensions across seeds\n",
        "        min_len = min([len(d) for d in data])\n",
        "        data_np = np.array([d[:min_len] for d in data])\n",
        "        x_data_np = np.array([d[:min_len] for d in x_data_list])\n",
        "\n",
        "        # Calculate Mean and Std Dev\n",
        "        mean_y = np.mean(data_np, axis=0)\n",
        "        mean_x = np.mean(x_data_np, axis=0)\n",
        "        std_y = np.std(data_np, axis=0)\n",
        "\n",
        "        # Plot\n",
        "        plt.plot(mean_x, mean_y, label=alg, color=colors[alg], linewidth=2)\n",
        "        plt.fill_between(mean_x, mean_y - (std_y/2), mean_y + (std_y/2), color=colors[alg], alpha=0.1)\n",
        "\n",
        "    # Adjust limits for time plots\n",
        "    if x_axis == 'time':\n",
        "        plt.xlim(0, 2.0) # Adjust this limit based on your actual run times\n",
        "\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.xlabel(\"Time (Seconds)\" if x_axis == 'time' else \"Epochs\", fontsize=12)\n",
        "    plt.ylabel(ylabel, fontsize=12)\n",
        "    plt.legend(fontsize=10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Save plot\n",
        "    filename = f\"PartA_{metric_key}_{x_axis}.png\"\n",
        "    plt.savefig(filename)\n",
        "    print(f\"Plot saved: {filename}\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nGenerating Plots...\")\n",
        "\n",
        "# Group 1: Epoch-based Metrics\n",
        "plot_metrics('train_loss', 'Training Loss vs Epochs', 'MSE Loss', x_axis='epoch')\n",
        "plot_metrics('test_loss', 'Test Loss vs Epochs', 'MSE Loss', x_axis='epoch')\n",
        "plot_metrics('train_acc', 'Training Accuracy vs Epochs', 'Accuracy', x_axis='epoch')\n",
        "plot_metrics('test_acc', 'Test Accuracy vs Epochs', 'Accuracy', x_axis='epoch')\n",
        "\n",
        "# Group 2: Time-based Metrics\n",
        "plot_metrics('train_loss', 'Training Loss vs Time', 'MSE Loss', x_axis='time')\n",
        "plot_metrics('test_loss', 'Test Loss vs Time', 'MSE Loss', x_axis='time')\n",
        "plot_metrics('train_acc', 'Training Accuracy vs Time', 'Accuracy', x_axis='time')\n",
        "plot_metrics('test_acc', 'Test Accuracy vs Time', 'Accuracy', x_axis='time')"
      ],
      "metadata": {
        "id": "As8uLnhtwA_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Data Preparation & t-SNE Dimensionality Reduction\n",
        "Loads the results from Part A, flattens weight histories, and computes 2D embeddings using t-SNE."
      ],
      "metadata": {
        "id": "zQsYzKb-wTRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Results\n",
        "print(\"1. Loading Part A Results...\")\n",
        "try:\n",
        "    results = np.load(\"model_results_part_A.npy\", allow_pickle=True).item()\n",
        "    print(\"-> Data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    raise SystemExit(\"ERROR: 'model_results_part_A.npy' not found. Please run Part A first.\")\n",
        "\n",
        "all_weights = []\n",
        "trajectory_map = [] # Renamed for clarity\n",
        "\n",
        "algorithms = ['GD', 'SGD', 'Adam']\n",
        "colors = {'GD': 'blue', 'SGD': 'orange', 'Adam': 'green'}\n",
        "\n",
        "idx_counter = 0\n",
        "\n",
        "# 2. Flatten Weights for t-SNE\n",
        "print(\"2. Processing Weight Histories...\")\n",
        "for alg in algorithms:\n",
        "    weight_histories = results[alg]['weights']\n",
        "\n",
        "    for seed_i, w_history in enumerate(weight_histories):\n",
        "        w_history = np.array(w_history)\n",
        "\n",
        "        # Append all steps to the master list\n",
        "        for w in w_history:\n",
        "            all_weights.append(w)\n",
        "\n",
        "        # Map indices to retrieve them later\n",
        "        trajectory_map.append({\n",
        "            'alg': alg,\n",
        "            'seed': seed_i,\n",
        "            'start': idx_counter,\n",
        "            'end': idx_counter + len(w_history)\n",
        "        })\n",
        "        idx_counter += len(w_history)\n",
        "\n",
        "# 3. Compute t-SNE\n",
        "print(\"3. Computing t-SNE (This may take a moment)...\")\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000, init='pca', learning_rate='auto')\n",
        "weights_embedded = tsne.fit_transform(np.array(all_weights))\n",
        "print(\"-> t-SNE Computation Complete.\")"
      ],
      "metadata": {
        "id": "ciODJ-UHwUfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Trajectory Visualization\n",
        "Visualizes the path taken by each optimization algorithm from 5 different starting points."
      ],
      "metadata": {
        "id": "f1ZnINJFwZ9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Tracker to ensure legend only appears once per algorithm\n",
        "legend_added = {'GD': False, 'SGD': False, 'Adam': False}\n",
        "\n",
        "print(\"4. Plotting Trajectories...\")\n",
        "\n",
        "# Plot each trajectory based on the map\n",
        "for info in trajectory_map:\n",
        "    alg = info['alg']\n",
        "    seed_id = info['seed']\n",
        "\n",
        "    # Extract coordinates for this specific run\n",
        "    path = weights_embedded[info['start']:info['end']]\n",
        "\n",
        "    # Handle Legend Label\n",
        "    lbl = alg if not legend_added[alg] else \"_nolegend_\"\n",
        "\n",
        "    # Plot Line (Trajectory)\n",
        "    plt.plot(path[:, 0], path[:, 1],\n",
        "             color=colors[alg],\n",
        "             linewidth=0.8,\n",
        "             alpha=0.4)\n",
        "\n",
        "    # Plot Points (Steps)\n",
        "    plt.scatter(path[:, 0], path[:, 1],\n",
        "             color=colors[alg],\n",
        "             s=15,\n",
        "             alpha=0.5,\n",
        "             label=lbl)\n",
        "\n",
        "    legend_added[alg] = True\n",
        "\n",
        "    # Mark Start Points (Stars)\n",
        "    # Only label start points for one algorithm (e.g., GD) to avoid clutter\n",
        "    if alg == 'GD':\n",
        "        plt.plot(path[0, 0], path[0, 1], marker='*', color='black', markersize=18, zorder=10)\n",
        "        plt.text(path[0, 0], path[0, 1], f\"  S{seed_id+1}\", fontsize=12, fontweight='bold', color='black')\n",
        "\n",
        "    # Mark End Points (X)\n",
        "    plt.plot(path[-1, 0], path[-1, 1], marker='X', color='black', markersize=10, zorder=5)\n",
        "\n",
        "# Styling\n",
        "plt.title(\"2D t-SNE Trajectories of Optimization Algorithms\", fontsize=16)\n",
        "plt.xlabel(\"Dimension 1\", fontsize=12)\n",
        "plt.ylabel(\"Dimension 2\", fontsize=12)\n",
        "plt.legend(fontsize=12, loc='best', title=\"Algorithms\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add explanatory text\n",
        "plt.figtext(0.5, 0.01, \"* Black Stars (S1-S5): 5 Different Initializations | Lines: Optimization Paths | Black X: Final Weights\",\n",
        "            ha=\"center\", fontsize=11, style='italic')\n",
        "\n",
        "filename = \"PartB_Combined_Trajectory.png\"\n",
        "plt.savefig(filename, dpi=300)\n",
        "print(f\"-> Plot saved: {filename}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kkIS8LmHwcs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Bonus: Algorithm Comparison on 2-Layer MLP\n",
        "Evaluates the performance of GD, SGD, and Adam optimizers on a non-linear architecture (Multi-Layer Perceptron) with ReLU activation."
      ],
      "metadata": {
        "id": "6Sd_8vR8wfMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data\n",
        "try:\n",
        "    X_train = np.load(\"egitim_X.npy\")\n",
        "    y_train = np.load(\"egitim_y.npy\")\n",
        "    X_test = np.load(\"test_X.npy\")\n",
        "    y_test = np.load(\"test_y.npy\")\n",
        "except FileNotFoundError:\n",
        "    raise SystemExit(\"ERROR: Data files (.npy) not found.\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Convert to Tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "y_train_tensor = torch.FloatTensor(y_train).view(-1, 1).to(device)\n",
        "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "y_test_tensor = torch.FloatTensor(y_test).view(-1, 1).to(device)\n",
        "\n",
        "print(f\"Data Prepared. Training Shape: {X_train_tensor.shape}\")\n",
        "\n",
        "# 2. Define MLP Model (2-Layer Neural Network)\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.layer2 = nn.Linear(hidden_dim, 1)\n",
        "        self.activation2 = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer2(x)\n",
        "        return self.activation2(x)\n",
        "\n",
        "# 3. Experiment Setup\n",
        "algorithms = ['Adam', 'SGD', 'GD']\n",
        "mlp_results = {alg: {'acc': [], 'loss': []} for alg in algorithms}\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "epochs = 100\n",
        "\n",
        "print(\"\\nStarting MLP Experiments...\")\n",
        "\n",
        "for alg in algorithms:\n",
        "    print(f\"   >>> Running {alg}...\")\n",
        "\n",
        "    # Reset model for each algorithm\n",
        "    model = MLPModel(input_dim, hidden_dim=64).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Optimizer & Batch Size Configuration\n",
        "    if alg == 'Adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        batch_size = len(X_train) # Full Batch for fairness comparison logic in this context\n",
        "    elif alg == 'SGD':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "        batch_size = 1 # Stochastic\n",
        "    elif alg == 'GD':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "        batch_size = len(X_train) # Full Batch\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        perm = torch.randperm(len(X_train))\n",
        "\n",
        "        # Mini-batch loop\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            indices = perm[i:i+batch_size]\n",
        "            batch_x, batch_y = X_train_tensor[indices], y_train_tensor[indices]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluation (Test Set)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_out = model(X_test_tensor)\n",
        "            # Accuracy calculation based on sign match\n",
        "            test_acc = (torch.sign(test_out) == y_test_tensor).float().mean().item()\n",
        "\n",
        "            mlp_results[alg]['loss'].append(loss.item())\n",
        "            mlp_results[alg]['acc'].append(test_acc)\n",
        "\n",
        "    print(f\"       -> {alg} Completed. Final Accuracy: {mlp_results[alg]['acc'][-1]*100:.1f}%\")\n",
        "\n",
        "# 4. Visualization\n",
        "plt.figure(figsize=(14, 6))\n",
        "colors = {'Adam': 'green', 'SGD': 'orange', 'GD': 'blue'}\n",
        "\n",
        "# Plot 1: Test Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "for alg in algorithms:\n",
        "    plt.plot(mlp_results[alg]['acc'], label=alg, color=colors[alg], linewidth=2)\n",
        "\n",
        "plt.axhline(y=0.5, color='gray', linestyle='--', label='Random Baseline (0.5)')\n",
        "plt.title(\"Algorithm Comparison on 2-Layer MLP (Accuracy)\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Test Accuracy\")\n",
        "plt.ylim(0, 1.05)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Training Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "for alg in algorithms:\n",
        "    plt.plot(mlp_results[alg]['loss'], label=alg, color=colors[alg], linewidth=2)\n",
        "\n",
        "plt.title(\"Training Loss (MSE) on MLP\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "filename = \"Bonus_MLP_Algorithm_Comparison.png\"\n",
        "plt.savefig(filename)\n",
        "print(f\"\\n-> Plot saved: {filename}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g9CkSjtGwtUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Bonus: Impact of Training Data Size (with Noise Injection)\n",
        "Analyzes how the model's generalization improves as the number of training samples increases. Gaussian noise is added to the training data to simulate real-world imperfections and highlight the importance of larger datasets."
      ],
      "metadata": {
        "id": "evHt9zgqw1Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Full Dataset\n",
        "print(\"1. Loading Vectors...\")\n",
        "try:\n",
        "    X_train_full = np.load(\"egitim_X.npy\")\n",
        "    y_train_full = np.load(\"egitim_y.npy\")\n",
        "    X_test = np.load(\"test_X.npy\")\n",
        "    y_test = np.load(\"test_y.npy\")\n",
        "except FileNotFoundError:\n",
        "    raise SystemExit(\"ERROR: .npy files not found.\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Prepare Test Set (Clean - No Noise)\n",
        "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "y_test_tensor = torch.FloatTensor(y_test).view(-1, 1).to(device)\n",
        "\n",
        "# 2. Define Simple Model (Same as Part A)\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1, bias=False)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.linear(x))\n",
        "\n",
        "# 3. Experiment Settings\n",
        "dataset_sizes = [5, 10, 20, 30, 40, 50, 60, 80, 100] # Number of samples to use\n",
        "avg_accuracies = []\n",
        "NOISE_LEVEL = 0.3 # Noise intensity to make the task harder\n",
        "\n",
        "print(f\"\\n--- Analyzing Effect of Training Set Size (Noise={NOISE_LEVEL}) ---\")\n",
        "\n",
        "input_dim = X_train_full.shape[1]\n",
        "\n",
        "for size in dataset_sizes:\n",
        "    current_accs = []\n",
        "\n",
        "    # Run 10 trials for each size to get a stable average\n",
        "    for _ in range(10):\n",
        "        # Randomly sample 'size' examples from training data\n",
        "        indices = np.random.choice(len(X_train_full), size, replace=False)\n",
        "        X_subset_np = X_train_full[indices]\n",
        "        y_subset_np = y_train_full[indices]\n",
        "\n",
        "        # Inject Noise (Training data is noisy)\n",
        "        noise = np.random.normal(loc=0.0, scale=NOISE_LEVEL, size=X_subset_np.shape)\n",
        "        X_subset_noisy = X_subset_np + noise\n",
        "\n",
        "        # Convert to Tensor\n",
        "        X_curr = torch.FloatTensor(X_subset_noisy).to(device)\n",
        "        y_curr = torch.FloatTensor(y_subset_np).view(-1, 1).to(device)\n",
        "\n",
        "        # Initialize and Train Model\n",
        "        model = SimpleModel(input_dim).to(device)\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        model.train()\n",
        "        for _ in range(100):\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(X_curr), y_curr)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Test Model (Test data is clean)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            preds = torch.sign(model(X_test_tensor))\n",
        "            acc = (preds == y_test_tensor).float().mean().item()\n",
        "            current_accs.append(acc)\n",
        "\n",
        "    # Average accuracy for this size\n",
        "    avg_acc = np.mean(current_accs)\n",
        "    avg_accuracies.append(avg_acc)\n",
        "    print(f\"Data Size: {size} -> Average Accuracy: {avg_acc*100:.1f}%\")\n",
        "\n",
        "# 4. Visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(dataset_sizes, avg_accuracies, 'o-', color='crimson', linewidth=3, label=f\"Test Accuracy (Noise={NOISE_LEVEL})\")\n",
        "\n",
        "plt.axhline(y=0.5, color='gray', linestyle='--', label=\"Random Baseline (0.5)\")\n",
        "\n",
        "plt.title(\"Effect of Training Set Size on Generalization\")\n",
        "plt.xlabel(\"Number of Training Samples\")\n",
        "plt.ylabel(\"Test Accuracy\")\n",
        "plt.ylim(0.4, 1.05)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "filename = \"Bonus_DataSize_WithNoise.png\"\n",
        "plt.savefig(filename)\n",
        "print(f\"\\n-> Plot saved: {filename}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fToOaDJSxJYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Bonus: Comprehensive Optimizer Benchmarking\n",
        "Expands the comparison to include Adagrad and RMSProp alongside GD, SGD, and Adam. This experiment uses a conservative learning rate to observe the convergence stability of adaptive methods vs. standard methods."
      ],
      "metadata": {
        "id": "Lg3bSyzXxNV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data\n",
        "try:\n",
        "    X_train = np.load(\"egitim_X.npy\")\n",
        "    y_train = np.load(\"egitim_y.npy\")\n",
        "    X_test = np.load(\"test_X.npy\")\n",
        "    y_test = np.load(\"test_y.npy\")\n",
        "except FileNotFoundError:\n",
        "    raise SystemExit(\"ERROR: .npy files not found.\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Convert to Tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train).to(device)\n",
        "y_train_tensor = torch.FloatTensor(y_train).view(-1, 1).to(device)\n",
        "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
        "y_test_tensor = torch.FloatTensor(y_test).view(-1, 1).to(device)\n",
        "\n",
        "print(f\"Data Ready: {X_train_tensor.shape}\")\n",
        "\n",
        "# 2. Define Model (Single Layer Linear + Tanh)\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1, bias=False)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.linear(x))\n",
        "\n",
        "# 3. Experiment Setup\n",
        "colors = {\n",
        "    'GD': 'blue',\n",
        "    'SGD': 'orange',\n",
        "    'Adagrad': 'purple',\n",
        "    'RMSProp': 'red',\n",
        "    'Adam': 'green'\n",
        "}\n",
        "\n",
        "algorithms = ['GD', 'SGD', 'Adagrad', 'RMSProp', 'Adam']\n",
        "results = {alg: {'acc': [], 'loss': []} for alg in algorithms}\n",
        "\n",
        "print(f\"\\n--- Starting 5-Way Algorithm Comparison ---\")\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "epochs = 100\n",
        "\n",
        "for alg in algorithms:\n",
        "    print(f\"\\n>>> Running {alg}...\")\n",
        "\n",
        "    # Reset Model for fairness\n",
        "    model = SimpleModel(input_dim).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Optimizer Configuration\n",
        "    # Note: Using a conservative learning rate (0.0001) for observation\n",
        "    if alg == 'GD':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
        "        batch_size = len(X_train) # Full Batch\n",
        "\n",
        "    elif alg == 'SGD':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
        "        batch_size = 1 # Stochastic\n",
        "\n",
        "    elif alg == 'Adagrad':\n",
        "        optimizer = torch.optim.Adagrad(model.parameters(), lr=0.0001)\n",
        "        batch_size = len(X_train)\n",
        "\n",
        "    elif alg == 'RMSProp':\n",
        "        optimizer = torch.optim.RMSprop(model.parameters(), lr=0.0001)\n",
        "        batch_size = len(X_train)\n",
        "\n",
        "    elif alg == 'Adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "        batch_size = len(X_train)\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        perm = torch.randperm(len(X_train))\n",
        "\n",
        "        # Batch Processing\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            indices = perm[i:i+batch_size]\n",
        "            batch_x, batch_y = X_train_tensor[indices], y_train_tensor[indices]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_out = model(X_test_tensor)\n",
        "            # Accuracy based on sign match\n",
        "            test_acc = (torch.sign(test_out) == y_test_tensor).float().mean().item()\n",
        "\n",
        "            results[alg]['loss'].append(loss.item())\n",
        "            results[alg]['acc'].append(test_acc)\n",
        "\n",
        "    print(f\"   -> {alg} Finished. Final Accuracy: {results[alg]['acc'][-1]*100:.1f}%\")\n",
        "\n",
        "# 4. Visualization\n",
        "plt.figure(figsize=(16, 6))\n",
        "\n",
        "# Subplot 1: Test Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "for alg in algorithms:\n",
        "    plt.plot(results[alg]['acc'], label=alg, color=colors[alg], linewidth=2.5, alpha=0.8)\n",
        "\n",
        "plt.axhline(y=0.5, color='gray', linestyle='--', label='Random Baseline (0.5)')\n",
        "plt.title(\"Optimizer Comparison (Accuracy)\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.ylim(0.4, 1.05)\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: Training Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "for alg in algorithms:\n",
        "    plt.plot(results[alg]['loss'], label=alg, color=colors[alg], linewidth=2.5, alpha=0.8)\n",
        "\n",
        "plt.title(\"Training Loss (MSE)\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "filename = \"Bonus_Optimization_Benchmark_Slow.png\"\n",
        "plt.savefig(filename)\n",
        "print(f\"\\n-> Plot saved: {filename}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cJJi6tMdxcP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10. Bonus: Semantic Representation Benchmarking (NLP)\n",
        "Compares traditional statistical methods (TF-IDF) against modern Deep Learning embeddings (Turkish-BERT and Turkish-E5) on a text classification task.\n",
        "\n",
        "TF-IDF: Frequency-based statistical vectorization.\n",
        "\n",
        "BERT: Contextual embeddings from dbmdz/bert-base-turkish-cased.\n",
        "\n",
        "E5: State-of-the-art semantic embeddings from ytu-ce-cosmos/turkish-e5-large."
      ],
      "metadata": {
        "id": "45S3yYGYxuXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load Data\n",
        "print(\"1. Loading and Processing Text Data...\")\n",
        "try:\n",
        "    df_train = pd.read_csv(\"egitim_seti.csv\")\n",
        "    df_test = pd.read_csv(\"test_seti.csv\")\n",
        "\n",
        "    # Combine Question and Answer\n",
        "    train_texts = [f\"Question: {r['Soru']} Answer: {r['Cevap']}\" for _, r in df_train.iterrows()]\n",
        "    test_texts = [f\"Question: {r['Soru']} Answer: {r['Cevap']}\" for _, r in df_test.iterrows()]\n",
        "\n",
        "    # Targets\n",
        "    y_train = torch.FloatTensor(df_train['Etiket'].values).view(-1, 1)\n",
        "    y_test = torch.FloatTensor(df_test['Etiket'].values).view(-1, 1)\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"ERROR: 'egitim_seti.csv' or 'test_seti.csv' not found.\")\n",
        "    sys.exit()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "y_train = y_train.to(device)\n",
        "y_test = y_test.to(device)\n",
        "\n",
        "results_data = {}\n",
        "\n",
        "# --- MODEL 1: TF-IDF ---\n",
        "print(\"\\n>>> Feature Extraction 1: TF-IDF...\")\n",
        "vectorizer = TfidfVectorizer(max_features=2000)\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(train_texts).toarray()\n",
        "X_test_tfidf = vectorizer.transform(test_texts).toarray()\n",
        "\n",
        "results_data['TF-IDF'] = {\n",
        "    'train': torch.FloatTensor(X_train_tfidf).to(device),\n",
        "    'test': torch.FloatTensor(X_test_tfidf).to(device),\n",
        "    'dim': X_train_tfidf.shape[1]\n",
        "}\n",
        "print(f\"   -> TF-IDF Dimension: {results_data['TF-IDF']['dim']}\")\n",
        "\n",
        "# --- MODEL 2: Turkish-BERT ---\n",
        "print(\"\\n>>> Feature Extraction 2: Turkish-BERT...\")\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
        "bert_model = AutoModel.from_pretrained(\"dbmdz/bert-base-turkish-cased\").to(device)\n",
        "\n",
        "def get_bert_embeddings(text_list):\n",
        "    embeddings = []\n",
        "    batch_size = 32\n",
        "    for i in range(0, len(text_list), batch_size):\n",
        "        batch = text_list[i:i+batch_size]\n",
        "        inputs = bert_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(**inputs)\n",
        "            cls_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "        embeddings.append(cls_emb)\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "X_train_bert = get_bert_embeddings(train_texts)\n",
        "X_test_bert = get_bert_embeddings(test_texts)\n",
        "\n",
        "results_data['BERT'] = {\n",
        "    'train': torch.FloatTensor(X_train_bert).to(device),\n",
        "    'test': torch.FloatTensor(X_test_bert).to(device),\n",
        "    'dim': X_train_bert.shape[1]\n",
        "}\n",
        "print(f\"   -> BERT Dimension: {results_data['BERT']['dim']}\")\n",
        "\n",
        "# --- MODEL 3: Turkish-E5 ---\n",
        "print(\"\\n>>> Feature Extraction 3: Turkish-E5...\")\n",
        "e5_model = SentenceTransformer('ytu-ce-cosmos/turkish-e5-large')\n",
        "\n",
        "X_train_e5 = e5_model.encode(train_texts)\n",
        "X_test_e5 = e5_model.encode(test_texts)\n",
        "\n",
        "results_data['E5'] = {\n",
        "    'train': torch.FloatTensor(X_train_e5).to(device),\n",
        "    'test': torch.FloatTensor(X_test_e5).to(device),\n",
        "    'dim': X_train_e5.shape[1]\n",
        "}\n",
        "print(f\"   -> E5 Dimension: {results_data['E5']['dim']}\")\n",
        "\n",
        "# --- Training ---\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.linear(x))\n",
        "\n",
        "final_scores = {}\n",
        "\n",
        "for model_name in ['TF-IDF', 'BERT', 'E5']:\n",
        "    print(f\"Training on: {model_name}...\")\n",
        "\n",
        "    data = results_data[model_name]\n",
        "    model = Classifier(data['dim']).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    acc_history = []\n",
        "\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data['train'])\n",
        "        loss = criterion(out, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Test\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            preds = torch.sign(model(data['test']))\n",
        "            acc = (preds == y_test).float().mean().item()\n",
        "            acc_history.append(acc)\n",
        "\n",
        "    final_scores[model_name] = acc_history\n",
        "    print(f\"   -> {model_name} Final Accuracy: {acc_history[-1]*100:.1f}%\")\n",
        "\n",
        "# --- Plotting ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "colors = {'TF-IDF': 'gray', 'BERT': 'orange', 'E5': 'green'}\n",
        "styles = {'TF-IDF': '--', 'BERT': '-.', 'E5': '-'}\n",
        "\n",
        "for name, scores in final_scores.items():\n",
        "    plt.plot(scores, label=f\"{name} (Final: {scores[-1]*100:.0f}%)\",\n",
        "             color=colors[name], linestyle=styles[name], linewidth=2.5)\n",
        "\n",
        "plt.axhline(y=0.5, color='red', linestyle=':', label=\"Random Baseline\")\n",
        "plt.title(\"Comparison of Text Representation Models\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Test Accuracy\")\n",
        "plt.ylim(0.4, 1.05)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.savefig(\"Bonus_Embedding_Comparison.png\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nPlot saved: Bonus_Embedding_Comparison.png\")"
      ],
      "metadata": {
        "id": "F7J7_JlBxwXx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}